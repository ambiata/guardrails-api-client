[@guardrails-ai/api-client](../README.md) / [Exports](../modules.md) / LLMResponse

# Interface: LLMResponse

Information from the LLM response.

**`Export`**

LLMResponse

## Table of contents

### Properties

- [asyncStreamOutput](LLMResponse.md#asyncstreamoutput)
- [output](LLMResponse.md#output)
- [promptTokenCount](LLMResponse.md#prompttokencount)
- [responseTokenCount](LLMResponse.md#responsetokencount)
- [streamOutput](LLMResponse.md#streamoutput)

## Properties

### asyncStreamOutput

• `Optional` **asyncStreamOutput**: `string`[]

**`Memberof`**

LLMResponse

#### Defined in

src/models/LLMResponse.ts:50

___

### output

• **output**: `string`

**`Memberof`**

LLMResponse

#### Defined in

src/models/LLMResponse.ts:38

___

### promptTokenCount

• `Optional` **promptTokenCount**: `number`

**`Memberof`**

LLMResponse

#### Defined in

src/models/LLMResponse.ts:26

___

### responseTokenCount

• `Optional` **responseTokenCount**: `number`

**`Memberof`**

LLMResponse

#### Defined in

src/models/LLMResponse.ts:32

___

### streamOutput

• `Optional` **streamOutput**: `string`[]

**`Memberof`**

LLMResponse

#### Defined in

src/models/LLMResponse.ts:44
